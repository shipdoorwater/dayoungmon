#!/usr/bin/env python3
"""
ë¡œì»¬ AI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.core.file_processor import FileProcessor, preprocess_text
from src.core.regulation_checker import RegulationChecker
from src.core.local_ai_analyzer import LocalAIAnalyzer

def test_local_ai_functionality():
    """ë¡œì»¬ AI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=== ë¡œì»¬ AI (Ollama) ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===\n")
    
    # ì´ˆê¸°í™”
    file_processor = FileProcessor()
    regulation_checker = RegulationChecker()
    local_ai_analyzer = LocalAIAnalyzer()
    
    print("1. ë¡œì»¬ AI í™˜ê²½ í™•ì¸")
    print(f"   Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬: {'âœ… ì„¤ì¹˜ë¨' if local_ai_analyzer.ollama_available else 'âŒ ì—†ìŒ'}")
    print(f"   Ollama ì„œë¹„ìŠ¤: {'âœ… ì‹¤í–‰ì¤‘' if local_ai_analyzer.ollama_running else 'âŒ ì¤‘ì§€'}")
    print(f"   ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {'âœ… ê°€ëŠ¥' if local_ai_analyzer.is_available() else 'âŒ ë¶ˆê°€ëŠ¥'}")
    
    if local_ai_analyzer.available_models:
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(local_ai_analyzer.available_models)}")
        print(f"   ì„ íƒëœ ëª¨ë¸: {local_ai_analyzer.get_best_model()}")
    else:
        print("   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: ì—†ìŒ")
    
    if not local_ai_analyzer.is_available():
        print("\n=== ì„¤ì¹˜ ê°€ì´ë“œ ===")
        guide = local_ai_analyzer.get_installation_guide()
        
        print("ì„¤ì¹˜ ë‹¨ê³„:")
        for step in guide['installation_steps']:
            print(f"   {step}")
        
        print("\nê¶Œì¥ ëª¨ë¸:")
        for model in guide['recommended_models']:
            print(f"   â€¢ {model}")
        
        print("\nì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­:")
        for key, value in guide['system_requirements'].items():
            print(f"   {key}: {value}")
        
        return
    
    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¡œë“œ
    sample_file = "assets/sample_text.txt"
    print(f"\n2. ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë¡œë“œ: {sample_file}")
    
    text = file_processor.extract_text(sample_file)
    if not text:
        print("âŒ í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    processed_text = preprocess_text(text)
    print(f"âœ… í…ìŠ¤íŠ¸ ë¡œë“œ ì„±ê³µ ({len(processed_text)}ì)")
    
    # ê¸°ë³¸ ê²€ì‚¬ ìˆ˜í–‰
    print("\n3. ê¸°ë³¸ í‚¤ì›Œë“œ ê²€ì‚¬ ìˆ˜í–‰")
    basic_violations = regulation_checker.check_violations(processed_text)
    basic_report = regulation_checker.generate_report(basic_violations, processed_text)
    
    print(f"   ê¸°ë³¸ ê²€ì‚¬ ê²°ê³¼: {len(basic_violations)}ê±´ ìœ„ë°˜ì‚¬í•­")
    print(f"   ìœ„í—˜ë„: {basic_report['risk_level']}")
    
    # ë¡œì»¬ AI ë¶„ì„ ìˆ˜í–‰
    print("\n4. ë¡œì»¬ AI ë¶„ì„ ìˆ˜í–‰ (ì‹œê°„ì´ ë§ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...)")
    try:
        local_ai_result = local_ai_analyzer.analyze_text(processed_text)
        
        if local_ai_result:
            print("âœ… ë¡œì»¬ AI ë¶„ì„ ì™„ë£Œ")
            print(f"   ì‚¬ìš©ëœ ëª¨ë¸: {local_ai_result.model_name}")
            print(f"   ë¡œì»¬ AI ìœ„ë°˜ì‚¬í•­: {len(local_ai_result.violations)}ê±´")
            print(f"   ì‹ ë¢°ë„: {local_ai_result.confidence_score:.1%}")
            print(f"   ì²˜ë¦¬ ì‹œê°„: {local_ai_result.processing_time:.1f}ì´ˆ")
            print(f"   ëª¨ë¸ í¬ê¸°: {local_ai_result.resource_usage.get('model_size', 'Unknown')}")
            
            # ì„±ëŠ¥ ë¹„êµ
            print("\n5. ì„±ëŠ¥ ë¹„êµ ë¶„ì„")
            print(f"   ê¸°ë³¸ ê²€ì‚¬: {len(basic_violations)}ê±´")
            print(f"   ë¡œì»¬ AI: {len(local_ai_result.violations)}ê±´")
            
            if len(local_ai_result.violations) > len(basic_violations):
                print("   ğŸ” ë¡œì»¬ AIê°€ ë” ë§ì€ ìœ„ë°˜ì‚¬í•­ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            elif len(local_ai_result.violations) < len(basic_violations):
                print("   ğŸ“Š ê¸°ë³¸ ê²€ì‚¬ê°€ ë” ë§ì€ ìœ„ë°˜ì‚¬í•­ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            else:
                print("   âš–ï¸ ë‘ ë°©ë²•ì˜ ìœ„ë°˜ì‚¬í•­ ê°œìˆ˜ê°€ ë™ì¼í•©ë‹ˆë‹¤.")
                
            # ì²˜ë¦¬ ì†ë„ ë¹„êµ
            basic_time = 0.1  # ê¸°ë³¸ ê²€ì‚¬ëŠ” ë§¤ìš° ë¹ ë¦„
            speedup = basic_time / local_ai_result.processing_time if local_ai_result.processing_time > 0 else float('inf')
            if speedup < 1:
                print(f"   â±ï¸ ë¡œì»¬ AIê°€ ê¸°ë³¸ ê²€ì‚¬ë³´ë‹¤ {1/speedup:.1f}ë°° ëŠë¦½ë‹ˆë‹¤.")
            else:
                print(f"   âš¡ ë¡œì»¬ AIê°€ ì˜ˆìƒë³´ë‹¤ ë¹ ë¦…ë‹ˆë‹¤!")
            
            # ë¡œì»¬ AI ë¶„ì„ ê²°ê³¼ ìƒì„¸ í‘œì‹œ
            print("\n6. ë¡œì»¬ AI ë¶„ì„ ìƒì„¸ ê²°ê³¼")
            print("=" * 50)
            print("ë§¥ë½ ë¶„ì„:")
            context_analysis = local_ai_result.contextual_analysis
            print(context_analysis[:200] + "..." if len(context_analysis) > 200 else context_analysis)
            
            print("\në²•ì  ìœ„í—˜ë„ í‰ê°€:")
            risk_assessment = local_ai_result.legal_risk_assessment
            print(risk_assessment[:200] + "..." if len(risk_assessment) > 200 else risk_assessment)
            
            print("\nì£¼ìš” ê°œì„  ì œì•ˆ:")
            for i, suggestion in enumerate(local_ai_result.improvement_suggestions[:3], 1):
                print(f"{i}. {suggestion}")
            
            # ë¡œì»¬ AI ë°œê²¬ ìœ„ë°˜ì‚¬í•­ ìƒ˜í”Œ
            print("\në¡œì»¬ AIê°€ ë°œê²¬í•œ ìœ„ë°˜ì‚¬í•­ (ì²˜ìŒ 3ê°œ):")
            for i, violation in enumerate(local_ai_result.violations[:3], 1):
                print(f"\n[{i}] {violation.get('text', 'N/A')}")
                print(f"    ìœ í˜•: {violation.get('type', 'N/A')}")
                print(f"    ì‹¬ê°ë„: {violation.get('severity', 'N/A')}")
                print(f"    ì œì•ˆ: {violation.get('suggestion', 'N/A')[:100]}...")
        else:
            print("âŒ ë¡œì»¬ AI ë¶„ì„ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ ë¡œì»¬ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸ëœ ë¦¬í¬íŠ¸
    print("\n7. ë¡œì»¬ AI ì‚¬ìš©ëŸ‰ ì •ë³´")
    usage_report = local_ai_analyzer.get_usage_report()
    print(f"   ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰: {usage_report['today_requests']}íšŒ")
    print(f"   ì´ ì‚¬ìš©ëŸ‰: {usage_report['total_requests']}íšŒ")
    print(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {usage_report['avg_processing_time']:.1f}ì´ˆ")
    print(f"   í˜„ì¬ ëª¨ë¸: {usage_report.get('current_model', 'N/A')}")
    
    print("\nâœ… ë¡œì»¬ AI ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ’¡ ë¡œì»¬ AIì˜ ì¥ì :")
    print("   â€¢ ì™„ì „ ì˜¤í”„ë¼ì¸ ì‘ë™ (ê°œì¸ì •ë³´ ë³´í˜¸)")
    print("   â€¢ ì‚¬ìš© í›„ ì¶”ê°€ ë¹„ìš© ì—†ìŒ")
    print("   â€¢ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¶ˆí•„ìš”")
    print("   â€¢ ìì²´ ì„œë²„ì—ì„œ ì‹¤í–‰")

def test_installation_guide():
    """ì„¤ì¹˜ ê°€ì´ë“œ í…ŒìŠ¤íŠ¸"""
    print("=== Ollama ì„¤ì¹˜ ê°€ì´ë“œ ===\n")
    
    local_ai_analyzer = LocalAIAnalyzer()
    guide = local_ai_analyzer.get_installation_guide()
    
    print("í˜„ì¬ ìƒíƒœ:")
    print(f"   Ollama ì„¤ì¹˜ë¨: {'âœ…' if guide['ollama_installed'] else 'âŒ'}")
    print(f"   Ollama ì‹¤í–‰ì¤‘: {'âœ…' if guide['ollama_running'] else 'âŒ'}")
    print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(guide['available_models'])}ê°œ")
    
    if guide['available_models']:
        for model in guide['available_models']:
            print(f"      - {model}")
    
    print("\nì„¤ì¹˜ ë‹¨ê³„:")
    for step in guide['installation_steps']:
        print(f"   {step}")
    
    print("\nê¶Œì¥ ëª¨ë¸:")
    for model in guide['recommended_models']:
        print(f"   â€¢ {model}")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--guide":
        test_installation_guide()
    else:
        test_local_ai_functionality()